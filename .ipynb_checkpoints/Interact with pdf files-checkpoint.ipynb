{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6722d560",
   "metadata": {},
   "source": [
    "# Interaction with PDF files using Langchain\n",
    "- Text representation : OpenAI embedding\n",
    "- Mode : GPT-3.5 -turbo\n",
    "- Framework : Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d615a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.0.189-py3-none-any.whl (975 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.6/975.6 KB\u001b[0m \u001b[31m129.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7\n",
      "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m217.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/lib/python3/dist-packages (from langchain) (5.4.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/lib/python3/dist-packages (from langchain) (2.25.1)\n",
      "Collecting numexpr<3.0.0,>=2.8.4\n",
      "  Downloading numexpr-2.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (381 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.4/381.4 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tenacity<9.0.0,>=8.1.0\n",
      "  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2\n",
      "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 KB\u001b[0m \u001b[31m842.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m910.3 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting SQLAlchemy<3,>=1.4\n",
      "  Downloading SQLAlchemy-2.0.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m253.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /home/darshan/.local/lib/python3.10/site-packages (from langchain) (1.24.3)\n",
      "Collecting pydantic<2,>=1\n",
      "  Downloading pydantic-1.10.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m379.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m398.3 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 KB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer<4.0,>=2.0\n",
      "  Downloading charset_normalizer-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 KB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 KB\u001b[0m \u001b[31m851.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m875.4 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (21.2.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting typing-inspect>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.3.0\n",
      "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 KB\u001b[0m \u001b[31m557.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1\n",
      "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/darshan/.local/lib/python3.10/site-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (613 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m613.7/613.7 KB\u001b[0m \u001b[31m389.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /home/darshan/.local/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.8.3->langchain) (3.3)\n",
      "Installing collected packages: tenacity, pydantic, numexpr, mypy-extensions, multidict, marshmallow, greenlet, frozenlist, charset-normalizer, async-timeout, yarl, typing-inspect, SQLAlchemy, openapi-schema-pydantic, marshmallow-enum, aiosignal, dataclasses-json, aiohttp, langchain\n",
      "Successfully installed SQLAlchemy-2.0.15 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 charset-normalizer-3.1.0 dataclasses-json-0.5.7 frozenlist-1.3.3 greenlet-2.0.2 langchain-0.0.189 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 numexpr-2.8.4 openapi-schema-pydantic-1.2.4 pydantic-1.10.8 tenacity-8.2.2 typing-inspect-0.9.0 yarl-1.9.2\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting unstructured\n",
      "  Downloading unstructured-0.7.1-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m357.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting python-docx\n",
      "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m956.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting xlrd\n",
      "  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: lxml in /usr/lib/python3/dist-packages (from unstructured) (4.8.0)\n",
      "Collecting pdfminer.six\n",
      "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m441.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: markdown in /home/darshan/.local/lib/python3.10/site-packages (from unstructured) (3.4.3)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from unstructured) (2.25.1)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m515.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting msg-parser\n",
      "  Downloading msg_parser-1.2.0-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 KB\u001b[0m \u001b[31m788.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m660.0 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting argilla\n",
      "  Downloading argilla-1.8.0-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m689.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: chardet in /usr/lib/python3/dist-packages (from unstructured) (4.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 KB\u001b[0m \u001b[31m784.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow in /usr/lib/python3/dist-packages (from unstructured) (9.0.1)\n",
      "Collecting pypandoc\n",
      "  Downloading pypandoc-1.11-py3-none-any.whl (20 kB)\n",
      "Collecting python-pptx\n",
      "  Downloading python-pptx-0.6.21.tar.gz (10.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m36m0:00:01\u001b[0m0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting python-magic\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: pandas in /home/darshan/.local/lib/python3.10/site-packages (from unstructured) (2.0.1)\n",
      "Requirement already satisfied: pydantic>=1.10.7 in /home/darshan/.local/lib/python3.10/site-packages (from argilla->unstructured) (1.10.8)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: monotonic in /usr/lib/python3/dist-packages (from argilla->unstructured) (1.6)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /home/darshan/.local/lib/python3.10/site-packages (from argilla->unstructured) (4.65.0)\n",
      "Collecting typer<1.0.0,>=0.6.0\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 KB\u001b[0m \u001b[31m869.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting httpx<0.24,>=0.15\n",
      "  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting wrapt<1.15,>=1.13\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 KB\u001b[0m \u001b[31m772.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m619.4 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting deprecated~=1.2.0\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting rich<=13.0.1\n",
      "  Downloading rich-13.0.1-py3-none-any.whl (238 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.1/238.1 KB\u001b[0m \u001b[31m738.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m702.7 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/darshan/.local/lib/python3.10/site-packages (from argilla->unstructured) (23.1)\n",
      "Collecting numpy<1.24.0\n",
      "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting backoff\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->unstructured) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/darshan/.local/lib/python3.10/site-packages (from pandas->unstructured) (2.8.2)\n",
      "Requirement already satisfied: olefile>=0.46 in /usr/lib/python3/dist-packages (from msg-parser->unstructured) (0.46)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 KB\u001b[0m \u001b[31m604.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m639.1 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2023.5.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 KB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk->unstructured) (8.0.3)\n",
      "Collecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Collecting cryptography>=36.0.0\n",
      "  Downloading cryptography-41.0.1-cp37-abi3-manylinux_2_28_x86_64.whl (4.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m791.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /home/darshan/.local/lib/python3.10/site-packages (from pdfminer.six->unstructured) (3.1.0)\n",
      "Collecting XlsxWriter>=0.5.7\n",
      "  Downloading XlsxWriter-3.1.2-py3-none-any.whl (153 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cffi>=1.12\n",
      "  Downloading cffi-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.8/441.8 KB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting httpcore<0.17.0,>=0.15.0\n",
      "  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 KB\u001b[0m \u001b[31m759.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m670.5 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx<0.24,>=0.15->argilla->unstructured) (2020.6.20)\n",
      "Collecting rfc3986[idna2008]<2,>=1.3\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting sniffio\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/darshan/.local/lib/python3.10/site-packages (from pydantic>=1.10.7->argilla->unstructured) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->unstructured) (1.16.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/lib/python3/dist-packages (from rich<=13.0.1->argilla->unstructured) (2.11.2)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pycparser\n",
      "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 KB\u001b[0m \u001b[31m631.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting h11<0.15,>=0.13\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m724.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting anyio<5.0,>=3.0\n",
      "  Downloading anyio-3.7.0-py3-none-any.whl (80 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna in /usr/lib/python3/dist-packages (from rfc3986[idna2008]<2,>=1.3->httpx<0.24,>=0.15->argilla->unstructured) (3.3)\n",
      "Collecting exceptiongroup\n",
      "  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: python-docx, python-pptx\n",
      "  Building wheel for python-docx (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184507 sha256=da6f3a98c089f0e60b51f45c4fa0c13b3fb3b03ee2e0327b372df26f7420387a\n",
      "  Stored in directory: /home/darshan/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n",
      "  Building wheel for python-pptx (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-pptx: filename=python_pptx-0.6.21-py3-none-any.whl size=470951 sha256=8910750dc7e2463e737251376fa963454f66341b8e5a6b8638c139253f133374\n",
      "  Stored in directory: /home/darshan/.cache/pip/wheels/ea/dd/74/01b3ec7256a0800b99384e9a0f7620e358afc3a51a59bf9b49\n",
      "Successfully built python-docx python-pptx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: rfc3986, commonmark, XlsxWriter, xlrd, wrapt, typer, sniffio, rich, regex, python-magic, python-docx, pypandoc, pycparser, numpy, msg-parser, joblib, h11, exceptiongroup, et-xmlfile, backoff, python-pptx, pandas, openpyxl, nltk, deprecated, cffi, anyio, httpcore, cryptography, pdfminer.six, httpx, argilla, unstructured\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.0.1\n",
      "    Uninstalling pandas-2.0.1:\n",
      "      Successfully uninstalled pandas-2.0.1\n",
      "Successfully installed XlsxWriter-3.1.2 anyio-3.7.0 argilla-1.8.0 backoff-2.2.1 cffi-1.15.1 commonmark-0.9.1 cryptography-41.0.1 deprecated-1.2.14 et-xmlfile-1.1.0 exceptiongroup-1.1.1 h11-0.14.0 httpcore-0.16.3 httpx-0.23.3 joblib-1.2.0 msg-parser-1.2.0 nltk-3.8.1 numpy-1.23.5 openpyxl-3.1.2 pandas-1.5.3 pdfminer.six-20221105 pycparser-2.21 pypandoc-1.11 python-docx-0.8.11 python-magic-0.4.27 python-pptx-0.6.21 regex-2023.5.5 rfc3986-1.5.0 rich-13.0.1 sniffio-1.3.0 typer-0.9.0 unstructured-0.7.1 wrapt-1.14.1 xlrd-2.0.1\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain --upgrade\n",
    "! pip install unstructured\n",
    "# Version: 0.0.149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d3e92ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader\n",
    "from langchain.document_loaders import UnstructuredFileLoader, PyPDFLoader\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "# # Load environment variables\n",
    "# load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166d759",
   "metadata": {},
   "source": [
    "### Load your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b4a2d6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 120 documents\n"
     ]
    }
   ],
   "source": [
    "# loader = UnstructuredFileLoader(\"../data/s41467-022-29926-w.pdf\")\n",
    "loader = PyPDFLoader(\"../data/DE-FOA-0002792_HFTO_FOA_FINAL_Mod_0001.pdf\")\n",
    "texts = loader.load_and_split()\n",
    "print (f'Now you have {len(texts)} documents')\n",
    "        \n",
    "# loader = UnstructuredFileLoader(\"../data/article2read.txt\")\n",
    "# loader = UnstructuredPDFLoader(\"../data/field-guide-to-data-science.pdf\")\n",
    "# loader = OnlinePDFLoader(\"https://www.nature.com/articles/s41467-022-29926-w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a13b4ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Questions about this FOA? hftofoa@ee.doe.gov  \\nProblems with EERE Exchang e? Email EERE- ExchangeSupport@hq.doe.gov  Include FOA name and number in \\nsubject line.  \\nModifications  \\nAll modifications to the FOA are [HIGHLIGHTED]  in the body of the FOA.  \\nMod. No.  Date  Description of Modification  \\n1 9/8/2022 Edited the FFRDC eligibility (Section III.A. ii and footnote 59 ) to \\nindicate that for Topic 4 only core members of the M2FCT \\nconsortium are ineligible to apply as subrecipients. Core national \\nlaboratories in M2FCT include Los Alamos National Laboratory, \\nLawrence Berkeley National Laboratory, Argonne National Laboratory,  National Renewable Energy Laboratory, and Oak \\nRidge National Laboratory .' metadata={'source': '../data/DE-FOA-0002792_HFTO_FOA_FINAL_Mod_0001.pdf', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcdac23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4fd7c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document(s) in your data\n",
      "There are 38577 characters in your document\n"
     ]
    }
   ],
   "source": [
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[0].page_content)} characters in your document')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af9b604",
   "metadata": {},
   "source": [
    "### Chunk your data up into smaller documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fcd081c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 55 documents\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(data)\n",
    "print (f'Now you have {len(texts)} documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fb3c6f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "10777694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "import textwrap\n",
    "\n",
    "def get_response_from_query(db, query, k=4):\n",
    "    \"\"\"\n",
    "    gpt-3.5-turbo can handle up to 4097 tokens. Setting the chunksize to 1000 and k to 4 maximizes\n",
    "    the number of tokens to analyze.\n",
    "    \"\"\"\n",
    "\n",
    "    docs = db.similarity_search(query, k=k)\n",
    "    docs_page_content = \" \".join([d.page_content for d in docs])\n",
    "\n",
    "    chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "\n",
    "    # Template to use for the system message prompt\n",
    "    template = \"\"\"\n",
    "        You are a helpful assistant that that can answer questions about scientific research in {docs}.\n",
    "        \n",
    "        Only use the information provided to answer the question.\n",
    "        \n",
    "        If you feel like you don't have enough information to answer the question, say \"I don't know\".\n",
    "   \n",
    "        Your answers should be verbose and detailed.\n",
    "        \"\"\"\n",
    "\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "    # Human question prompt\n",
    "    human_template = \"Answer the following question: {question}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "    chat_prompt = ChatPromptTemplate.from_messages(\n",
    "        [system_message_prompt, human_message_prompt]\n",
    "    )\n",
    "\n",
    "    chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "\n",
    "    response = chain.run(question=query, docs=docs_page_content)\n",
    "    response = response.replace(\"\\n\", \"\")\n",
    "    \n",
    "    return response, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55e4ba61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EERE expects to make a total of approximately\n",
      "$60.5 million of federal funding available for new\n",
      "awards under this FOA, subject to the availability\n",
      "of appropriated funds. The anticipated federal\n",
      "funding for each of the five topic areas ranges\n",
      "from $8 million to $20 million, with the number of\n",
      "anticipated awards ranging from 1 to 27. The\n",
      "maximum DOE funding for each project proposal\n",
      "varies depending on the topic area, ranging from\n",
      "$8 million to $20 million. Applicants should\n",
      "propose projects with funding requests that are\n",
      "commensurate with the level of work proposed.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "query = \"What is the funding size of this research funding call?\"\n",
    "response, docs = get_response_from_query(db, query)\n",
    "\n",
    "print(textwrap.fill(response, width=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb6bb495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='ARTICLE\\nBreaking adsorption-energy scaling limitations of\\nelectrocatalytic nitrate reduction on intermetallic\\nCuPd nanocubes by machine-learned insights\\nQiang Gao1,4, Hemanth Somarajan Pillai1,4, Yang Huang1, Shikai Liu2, Qingmin Mu1, Xue Han1, Zihao Yan1,\\nHua Zhou3, Qian He2✉, Hongliang Xin1✉& Huiyuan Zhu1✉\\nThe electrochemical nitrate reduction reaction (NO 3RR) to ammonia is an essential step\\ntoward restoring the globally disrupted nitrogen cycle. In search of highly ef ﬁcient electro-\\ncatalysts, tailoring catalytic sites with ligand and strain effects in random alloys is a commonapproach but remains limited due to the ubiquitous energy-scaling relations. With inter-pretable machine learning, we unravel a mechanism of breaking adsorption-energy scalingrelations through the site-speci ﬁc Pauli repulsion interactions of the metal d-states with\\nadsorbate frontier orbitals. The non-scaling behavior can be realized on (100)-type sites ofordered B2 intermetallics, in which the orbital overlap between the hollow *N and subsurfacemetal atoms is signi ﬁcant while the bridge-bidentate *NO\\n3is not directly affected. Among\\nthose intermetallics predicted, we synthesize monodisperse ordered B2 CuPd nanocubes thatdemonstrate high performance for NO\\n3RR to ammonia with a Faradaic ef ﬁciency of 92.5% at\\n−0.5 V RHEand a yield rate of 6.25 mol h−1g−1at−0.6 V RHE. This study provides machine-\\nlearned design rules besides the d-band center metrics, paving the path toward data-driven\\ndiscovery of catalytic materials beyond linear scaling limitations.https://doi.org/10.1038/s41467-022-29926-w OPEN\\n1Department of Chemical Engineering, Virginia Polytechnic Institute and State University, 635 Prices Fork Rd., Blacksburg, VA 24061, USA.2Department of\\nMaterials Science and Engineering, National University of Singapore, 9 Engineering Drive 1, 117575 Singapore, Singapore.3X-ray Science Division, Advanced\\nPhoton Source, Argonne National Laboratory, Lemont, IL 60439, USA.4These authors contributed equally: Qiang Gao, Hemanth Somarajan Pillai.\\n✉email: heqian@nus.edu.sg ;hxin@vt.edu ;huiyuanz@vt.edu\\nNATURE COMMUNICATIONS |         (2022) 13:2338 | https://doi.org/10.1038/s41467-022-29926-w | www.nature.com/naturecommunications 11234567890():,;' metadata={'source': '../data/s41467-022-29926-w.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(docs[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
